{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from os import makedirs\n",
    "from os.path import join, isfile, basename\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed3dab7ee227c19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import List, Tuple, Type\n",
    "\n",
    "# from .common import LayerNorm2d\n",
    "\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: int,\n",
    "        transformer: nn.Module,\n",
    "        num_multimask_outputs: int = 3,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "        iou_head_depth: int = 3,\n",
    "        iou_head_hidden_dim: int = 256,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Predicts masks given an image and prompt embeddings, using a\n",
    "        transformer architecture.\n",
    "\n",
    "        Arguments:\n",
    "          transformer_dim (int): the channel dimension of the transformer\n",
    "          transformer (nn.Module): the transformer used to predict masks\n",
    "          num_multimask_outputs (int): the number of masks to predict\n",
    "            when disambiguating masks\n",
    "          activation (nn.Module): the type of activation to use when\n",
    "            upscaling masks\n",
    "          iou_head_depth (int): the depth of the MLP used to predict\n",
    "            mask quality\n",
    "          iou_head_hidden_dim (int): the hidden dimension of the MLP\n",
    "            used to predict mask quality\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.num_multimask_outputs = num_multimask_outputs\n",
    "\n",
    "        self.iou_token = nn.Embedding(1, transformer_dim)\n",
    "        self.num_mask_tokens = num_multimask_outputs + 1\n",
    "        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n",
    "\n",
    "        self.output_upscaling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                transformer_dim, transformer_dim // 4, kernel_size=2, stride=2\n",
    "            ),\n",
    "            LayerNorm2d(transformer_dim // 4),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(\n",
    "                transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2\n",
    "            ),\n",
    "            activation(),\n",
    "        )\n",
    "        self.output_hypernetworks_mlps = nn.ModuleList(\n",
    "            [\n",
    "                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n",
    "                for i in range(self.num_mask_tokens)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.iou_prediction_head = MLP(\n",
    "            transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "        dense_prompt_embeddings: torch.Tensor,\n",
    "        multimask_output: bool,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict masks given image and prompt embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          image_embeddings (torch.Tensor): the embeddings from the image encoder\n",
    "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
    "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
    "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
    "          multimask_output (bool): Whether to return multiple masks or a single\n",
    "            mask.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: batched predicted masks\n",
    "          torch.Tensor: batched predictions of mask quality\n",
    "        \"\"\"\n",
    "        masks, iou_pred = self.predict_masks(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=image_pe,\n",
    "            sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
    "            dense_prompt_embeddings=dense_prompt_embeddings,\n",
    "        )\n",
    "\n",
    "        # Select the correct mask or masks for output\n",
    "        if multimask_output:\n",
    "            mask_slice = slice(1, None)\n",
    "        else:\n",
    "            mask_slice = slice(0, 1)\n",
    "        masks = masks[:, mask_slice, :, :]\n",
    "        iou_pred = iou_pred[:, mask_slice]\n",
    "\n",
    "        # Prepare output\n",
    "        return masks, iou_pred\n",
    "\n",
    "    def predict_masks(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "        dense_prompt_embeddings: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n",
    "        # Concatenate output tokens\n",
    "        output_tokens = torch.cat(\n",
    "            [self.iou_token.weight, self.mask_tokens.weight], dim=0\n",
    "        )\n",
    "        output_tokens = output_tokens.unsqueeze(0).expand(\n",
    "            sparse_prompt_embeddings.size(0), -1, -1\n",
    "        )\n",
    "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
    "\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        if image_embeddings.shape[0] != tokens.shape[0]:\n",
    "            src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
    "        else:\n",
    "            src = image_embeddings\n",
    "        src = src + dense_prompt_embeddings\n",
    "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        # Run the transformer\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "        iou_token_out = hs[:, 0, :]\n",
    "        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n",
    "\n",
    "        # Upscale mask embeddings and predict masks using the mask tokens\n",
    "        src = src.transpose(1, 2).view(b, c, h, w)\n",
    "        upscaled_embedding = self.output_upscaling(src)\n",
    "        hyper_in_list: List[torch.Tensor] = []\n",
    "        for i in range(self.num_mask_tokens):\n",
    "            hyper_in_list.append(\n",
    "                self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])\n",
    "            )\n",
    "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        b, c, h, w = upscaled_embedding.shape\n",
    "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "\n",
    "        # Generate mask quality predictions\n",
    "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
    "\n",
    "        return masks, iou_pred\n",
    "\n",
    "\n",
    "# Lightly adapted from\n",
    "# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63cb534f85a5c015"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19efeac833a53bd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "import math\n",
    "from typing import Tuple, Type\n",
    "\n",
    "# from .common import MLPBlock\n",
    "\n",
    "\n",
    "class TwoWayTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer decoder that attends to an input image using\n",
    "        queries whose positional embedding is supplied.\n",
    "\n",
    "        Args:\n",
    "          depth (int): number of layers in the transformer\n",
    "          embedding_dim (int): the channel dimension for the input embeddings\n",
    "          num_heads (int): the number of heads for multihead attention. Must\n",
    "            divide embedding_dim\n",
    "          mlp_dim (int): the channel dimension internal to the MLP block\n",
    "          activation (nn.Module): the activation to use in the MLP block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(\n",
    "                TwoWayAttentionBlock(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    activation=activation,\n",
    "                    attention_downsample_rate=attention_downsample_rate,\n",
    "                    skip_first_layer_pe=(i == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm_final_attn = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embedding: Tensor,\n",
    "        image_pe: Tensor,\n",
    "        point_embedding: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_embedding (torch.Tensor): image to attend to. Should be shape\n",
    "            B x embedding_dim x h x w for any h and w.\n",
    "          image_pe (torch.Tensor): the positional encoding to add to the image. Must\n",
    "            have the same shape as image_embedding.\n",
    "          point_embedding (torch.Tensor): the embedding to add to the query points.\n",
    "            Must have shape B x N_points x embedding_dim for any N_points.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: the processed point_embedding\n",
    "          torch.Tensor: the processed image_embedding\n",
    "        \"\"\"\n",
    "        # BxCxHxW -> BxHWxC == B x N_image_tokens x C\n",
    "        bs, c, h, w = image_embedding.shape\n",
    "        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)\n",
    "        image_pe = image_pe.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # Prepare queries\n",
    "        queries = point_embedding\n",
    "        keys = image_embedding\n",
    "\n",
    "        # Apply transformer blocks and final layernorm\n",
    "        for layer in self.layers:\n",
    "            queries, keys = layer(\n",
    "                queries=queries,\n",
    "                keys=keys,\n",
    "                query_pe=point_embedding,\n",
    "                key_pe=image_pe,\n",
    "            )\n",
    "\n",
    "        # Apply the final attention layer from the points to the image\n",
    "        q = queries + point_embedding\n",
    "        k = keys + image_pe\n",
    "        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm_final_attn(queries)\n",
    "\n",
    "        return queries, keys\n",
    "\n",
    "\n",
    "class TwoWayAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_dim: int = 2048,\n",
    "        activation: Type[nn.Module] = nn.ReLU,\n",
    "        attention_downsample_rate: int = 2,\n",
    "        skip_first_layer_pe: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A transformer block with four layers: (1) self-attention of sparse\n",
    "        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp\n",
    "        block on sparse inputs, and (4) cross attention of dense inputs to sparse\n",
    "        inputs.\n",
    "\n",
    "        Arguments:\n",
    "          embedding_dim (int): the channel dimension of the embeddings\n",
    "          num_heads (int): the number of heads in the attention layers\n",
    "          mlp_dim (int): the hidden dimension of the mlp block\n",
    "          activation (nn.Module): the activation of the mlp block\n",
    "          skip_first_layer_pe (bool): skip the PE on the first layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embedding_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attn_token_to_image = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.norm4 = nn.LayerNorm(embedding_dim)\n",
    "        self.cross_attn_image_to_token = Attention(\n",
    "            embedding_dim, num_heads, downsample_rate=attention_downsample_rate\n",
    "        )\n",
    "\n",
    "        self.skip_first_layer_pe = skip_first_layer_pe\n",
    "\n",
    "    def forward(\n",
    "        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        # Self attention block\n",
    "        if self.skip_first_layer_pe:\n",
    "            queries = self.self_attn(q=queries, k=queries, v=queries)\n",
    "        else:\n",
    "            q = queries + query_pe\n",
    "            attn_out = self.self_attn(q=q, k=q, v=queries)\n",
    "            queries = queries + attn_out\n",
    "        queries = self.norm1(queries)\n",
    "\n",
    "        # Cross attention block, tokens attending to image embedding\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)\n",
    "        queries = queries + attn_out\n",
    "        queries = self.norm2(queries)\n",
    "\n",
    "        # MLP block\n",
    "        mlp_out = self.mlp(queries)\n",
    "        queries = queries + mlp_out\n",
    "        queries = self.norm3(queries)\n",
    "\n",
    "        # Cross attention block, image embedding attending to tokens\n",
    "        q = queries + query_pe\n",
    "        k = keys + key_pe\n",
    "        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)\n",
    "        keys = keys + attn_out\n",
    "        keys = self.norm4(keys)\n",
    "\n",
    "        return queries, keys\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.internal_dim % num_heads == 0\n",
    "        ), \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e753857ed964ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "\n",
    "# from .common import LayerNorm2d\n",
    "\n",
    "\n",
    "class PromptEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Encodes prompts for input to SAM's mask decoder.\n",
    "\n",
    "        Arguments:\n",
    "          embed_dim (int): The prompts' embedding dimension\n",
    "          image_embedding_size (tuple(int, int)): The spatial size of the\n",
    "            image embedding, as (H, W).\n",
    "          input_image_size (int): The padded size of the image as input\n",
    "            to the image encoder, as (H, W).\n",
    "          mask_in_chans (int): The number of hidden channels used for\n",
    "            encoding input masks.\n",
    "          activation (nn.Module): The activation to use when encoding\n",
    "            input masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.image_embedding_size = image_embedding_size\n",
    "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
    "\n",
    "        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
    "        point_embeddings = [\n",
    "            nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)\n",
    "        ]\n",
    "        self.point_embeddings = nn.ModuleList(point_embeddings)\n",
    "        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "        self.mask_input_size = (\n",
    "            4 * image_embedding_size[0],\n",
    "            4 * image_embedding_size[1],\n",
    "        )\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans // 4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "    def get_dense_pe(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the positional encoding used to encode point prompts,\n",
    "        applied to a dense set of points the shape of the image encoding.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: Positional encoding with shape\n",
    "            1x(embed_dim)x(embedding_h)x(embedding_w)\n",
    "        \"\"\"\n",
    "        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n",
    "\n",
    "    def _embed_points(\n",
    "        self,\n",
    "        points: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        pad: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Embeds point prompts.\"\"\"\n",
    "        points = points + 0.5  # Shift to center of pixel\n",
    "        if pad:\n",
    "            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n",
    "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n",
    "            points = torch.cat([points, padding_point], dim=1)\n",
    "            labels = torch.cat([labels, padding_label], dim=1)\n",
    "        point_embedding = self.pe_layer.forward_with_coords(\n",
    "            points, self.input_image_size\n",
    "        )\n",
    "        point_embedding[labels == -1] = 0.0\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds box prompts.\"\"\"\n",
    "        boxes = boxes + 0.5  # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.pe_layer.forward_with_coords(\n",
    "            coords, self.input_image_size\n",
    "        )\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
    "        return corner_embedding\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Gets the batch size of the output given the batch size of the input prompts.\n",
    "        \"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def _get_device(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "        \"\"\"\n",
    "        bs = self._get_batch_size(points, boxes, masks)\n",
    "        sparse_embeddings = torch.empty(\n",
    "            (bs, 0, self.embed_dim), device=self._get_device()\n",
    "        )\n",
    "        if points is not None:\n",
    "            coords, labels = points\n",
    "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "        if boxes is not None:\n",
    "            box_embeddings = self._embed_boxes(boxes)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else:\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "\n",
    "        return sparse_embeddings, dense_embeddings\n",
    "\n",
    "\n",
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d522f81c418f8aff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from tiny_vit_sam import TinyViT\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# TinyViT Model Architecture\n",
    "# Copyright (c) 2022 Microsoft\n",
    "# Adapted from LeViT and Swin Transformer\n",
    "#   LeViT: (https://github.com/facebookresearch/levit)\n",
    "#   Swin: (https://github.com/microsoft/swin-transformer)\n",
    "# Build the TinyViT Model\n",
    "# --------------------------------------------------------\n",
    "# The TinyViT model is adapted from MobileSAM's variant.\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath as TimmDropPath,\\\n",
    "    to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        bn = torch.nn.BatchNorm2d(b)\n",
    "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(bn.bias, 0)\n",
    "        self.add_module('bn', bn)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class DropPath(TimmDropPath):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__(drop_prob=drop_prob)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def __repr__(self):\n",
    "        msg = super().__repr__()\n",
    "        msg += f'(drop_prob={self.drop_prob})'\n",
    "        return msg\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans, embed_dim, resolution, activation):\n",
    "        super().__init__()\n",
    "        img_size: Tuple[int, int] = to_2tuple(resolution)\n",
    "        #self.patches_resolution = (img_size[0] // 4, img_size[1] // 4)\n",
    "        self.patches_resolution = img_size\n",
    "        self.num_patches = self.patches_resolution[0] * \\\n",
    "            self.patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        n = embed_dim\n",
    "        #self.seq = nn.Sequential(\n",
    "        #    Conv2d_BN(in_chans, n // 2, 3, 2, 1),\n",
    "        #    activation(),\n",
    "        #    Conv2d_BN(n // 2, n, 3, 2, 1),\n",
    "        #)\n",
    "        self.seq = nn.Sequential(\n",
    "            Conv2d_BN(in_chans, n // 2, 1, 1, 0),\n",
    "            activation(),\n",
    "            Conv2d_BN(n // 2, n, 1, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, expand_ratio,\n",
    "                 activation, drop_path):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.hidden_chans = int(in_chans * expand_ratio)\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\n",
    "        self.act1 = activation()\n",
    "\n",
    "        self.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans,\n",
    "                               ks=3, stride=1, pad=1, groups=self.hidden_chans)\n",
    "        self.act2 = activation()\n",
    "\n",
    "        self.conv3 = Conv2d_BN(\n",
    "            self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)\n",
    "        self.act3 = activation()\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.drop_path(x)\n",
    "\n",
    "        x += shortcut\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, out_dim, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.act = activation()\n",
    "        self.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)\n",
    "        stride_c=2\n",
    "        if(out_dim==320 or out_dim==448 or out_dim==576):\n",
    "            stride_c=1\n",
    "        self.conv2 = Conv2d_BN(out_dim, out_dim, 3, stride_c, 1, groups=out_dim)\n",
    "        self.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:\n",
    "            H, W = self.input_resolution\n",
    "            B = len(x)\n",
    "            # (B, C, H, W)\n",
    "            x = x.view(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, depth,\n",
    "                 activation,\n",
    "                 drop_path=0., downsample=None, use_checkpoint=False,\n",
    "                 out_dim=None,\n",
    "                 conv_expand_ratio=4.,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MBConv(dim, dim, conv_expand_ratio, activation,\n",
    "                   drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                   )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.norm = nn.LayerNorm(in_features)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=(14, 14),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # (h, w)\n",
    "        assert isinstance(resolution, tuple) and len(resolution) == 2\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.nh_kd = nh_kd = key_dim * num_heads\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
    "        self.attn_ratio = attn_ratio\n",
    "        h = self.dh + nh_kd * 2\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.qkv = nn.Linear(dim, h)\n",
    "        self.proj = nn.Linear(self.dh, dim)\n",
    "\n",
    "        points = list(itertools.product(\n",
    "            range(resolution[0]), range(resolution[1])))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N),\n",
    "                             persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.register_buffer('ab',\n",
    "                                 self.attention_biases[:, self.attention_bias_idxs],\n",
    "                                 persistent=False)\n",
    "\n",
    "    def forward(self, x):  # x (B,N,C)\n",
    "        B, N, _ = x.shape\n",
    "\n",
    "        # Normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        # (B, N, num_heads, d)\n",
    "        q, k, v = qkv.view(B, N, self.num_heads, -\n",
    "                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n",
    "        # (B, num_heads, N, d)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (\n",
    "            (q @ k.transpose(-2, -1)) * self.scale\n",
    "            +\n",
    "            (self.attention_biases[:, self.attention_bias_idxs]\n",
    "             if self.training else self.ab)\n",
    "        )\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyViTBlock(nn.Module):\n",
    "    r\"\"\" TinyViT Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int, int]): Input resolution.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        local_conv_size (int): the kernel size of the convolution between\n",
    "                               Attention and MLP. Default: 3\n",
    "        activation: the activation function. Default: nn.GELU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7,\n",
    "                 mlp_ratio=4., drop=0., drop_path=0.,\n",
    "                 local_conv_size=3,\n",
    "                 activation=nn.GELU,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        assert window_size > 0, 'window_size must be greater than 0'\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        assert dim % num_heads == 0, 'dim must be divisible by num_heads'\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        window_resolution = (window_size, window_size)\n",
    "        self.attn = Attention(dim, head_dim, num_heads,\n",
    "                              attn_ratio=1, resolution=window_resolution)\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        mlp_activation = activation\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=mlp_activation, drop=drop)\n",
    "\n",
    "        pad = local_conv_size // 2\n",
    "        self.local_conv = Conv2d_BN(\n",
    "            dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        res_x = x\n",
    "        if H == self.window_size and W == self.window_size:\n",
    "            x = self.attn(x)\n",
    "        else:\n",
    "            x = x.view(B, H, W, C)\n",
    "            pad_b = (self.window_size - H %\n",
    "                     self.window_size) % self.window_size\n",
    "            pad_r = (self.window_size - W %\n",
    "                     self.window_size) % self.window_size\n",
    "            padding = pad_b > 0 or pad_r > 0\n",
    "\n",
    "            if padding:\n",
    "                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "\n",
    "            pH, pW = H + pad_b, W + pad_r\n",
    "            nH = pH // self.window_size\n",
    "            nW = pW // self.window_size\n",
    "            # window partition\n",
    "            x = x.view(B, nH, self.window_size, nW, self.window_size, C).transpose(2, 3).reshape(\n",
    "                B * nH * nW, self.window_size * self.window_size, C)\n",
    "            x = self.attn(x)\n",
    "            # window reverse\n",
    "            x = x.view(B, nH, nW, self.window_size, self.window_size,\n",
    "                       C).transpose(2, 3).reshape(B, pH, pW, C)\n",
    "\n",
    "            if padding:\n",
    "                x = x[:, :H, :W].contiguous()\n",
    "\n",
    "            x = x.view(B, L, C)\n",
    "\n",
    "        x = res_x + self.drop_path(x)\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "        x = self.local_conv(x)\n",
    "        x = x.view(B, C, L).transpose(1, 2)\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic TinyViT layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        local_conv_size: the kernel size of the depthwise convolution between attention and MLP. Default: 3\n",
    "        activation: the activation function. Default: nn.GELU\n",
    "        out_dim: the output dimension of the layer. Default: dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., drop=0.,\n",
    "                 drop_path=0., downsample=None, use_checkpoint=False,\n",
    "                 local_conv_size=3,\n",
    "                 activation=nn.GELU,\n",
    "                 out_dim=None,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TinyViTBlock(dim=dim, input_resolution=input_resolution,\n",
    "                         num_heads=num_heads, window_size=window_size,\n",
    "                         mlp_ratio=mlp_ratio,\n",
    "                         drop=drop,\n",
    "                         drop_path=drop_path[i] if isinstance(\n",
    "                             drop_path, list) else drop_path,\n",
    "                         local_conv_size=local_conv_size,\n",
    "                         activation=activation,\n",
    "                         )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 in_chans=3,\n",
    "                 #num_classes=1000,\n",
    "                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n",
    "                 num_heads=[3, 6, 12, 24],\n",
    "                 window_sizes=[7, 7, 14, 7],\n",
    "                 mlp_ratio=4.,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 use_checkpoint=False,\n",
    "                 mbconv_expand_ratio=4.0,\n",
    "                 local_conv_size=3,\n",
    "                 layer_lr_decay=1.0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.img_size=img_size\n",
    "        #self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_layers = len(depths)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        activation = nn.GELU\n",
    "\n",
    "        self.patch_embed = PatchEmbed(in_chans=in_chans,\n",
    "                                      embed_dim=embed_dims[0],\n",
    "                                      resolution=img_size,\n",
    "                                      activation=activation)\n",
    "\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate,\n",
    "                                                sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            kwargs = dict(dim=embed_dims[i_layer],\n",
    "                        input_resolution=(\n",
    "                            patches_resolution[0] // (2 ** (i_layer-1 if i_layer == 3 else i_layer)),\n",
    "                            patches_resolution[1] // (2 ** (i_layer-1 if i_layer == 3 else i_layer))\n",
    "                        ),\n",
    "                        #   input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                        #                     patches_resolution[1] // (2 ** i_layer)),\n",
    "                          depth=depths[i_layer],\n",
    "                          drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                          downsample=PatchMerging if (\n",
    "                              i_layer < self.num_layers - 1) else None,\n",
    "                          use_checkpoint=use_checkpoint,\n",
    "                          out_dim=embed_dims[min(\n",
    "                              i_layer + 1, len(embed_dims) - 1)],\n",
    "                          activation=activation,\n",
    "                          )\n",
    "            if i_layer == 0:\n",
    "                layer = ConvLayer(\n",
    "                    conv_expand_ratio=mbconv_expand_ratio,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "                layer = BasicLayer(\n",
    "                    num_heads=num_heads[i_layer],\n",
    "                    window_size=window_sizes[i_layer],\n",
    "                    mlp_ratio=self.mlp_ratio,\n",
    "                    drop=drop_rate,\n",
    "                    local_conv_size=local_conv_size,\n",
    "                    **kwargs)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        self.set_layer_lr_decay(layer_lr_decay)\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dims[-1],\n",
    "                256,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(256),\n",
    "            nn.Conv2d(\n",
    "                256,\n",
    "                256,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(256),\n",
    "        )\n",
    "\n",
    "    def set_layer_lr_decay(self, layer_lr_decay):\n",
    "        decay_rate = layer_lr_decay\n",
    "\n",
    "        # layers -> blocks (depth)\n",
    "        depth = sum(self.depths)\n",
    "        lr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]\n",
    "\n",
    "        def _set_lr_scale(m, scale):\n",
    "            for p in m.parameters():\n",
    "                p.lr_scale = scale\n",
    "\n",
    "        self.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            for block in layer.blocks:\n",
    "                block.apply(lambda x: _set_lr_scale(x, lr_scales[i]))\n",
    "                i += 1\n",
    "            if layer.downsample is not None:\n",
    "                layer.downsample.apply(\n",
    "                    lambda x: _set_lr_scale(x, lr_scales[i - 1]))\n",
    "        assert i == depth\n",
    "\n",
    "        for k, p in self.named_parameters():\n",
    "            p.param_name = k\n",
    "\n",
    "        def _check_lr_scale(m):\n",
    "            for p in m.parameters():\n",
    "                assert hasattr(p, 'lr_scale'), p.param_name\n",
    "\n",
    "        self.apply(_check_lr_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'attention_biases'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # x: (N, C, H, W)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x = self.layers[0](x)\n",
    "        start_i = 1\n",
    "\n",
    "        for i in range(start_i, len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = layer(x)\n",
    "\n",
    "        B, _, C = x.size()\n",
    "        x = x.view(B, 64, 64, C)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.neck(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8704a88adb935664"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.45])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.45])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "\n",
    "def cal_iou(result, reference):\n",
    "    intersection = torch.count_nonzero(torch.logical_and(result, reference), dim=[i for i in range(1, result.ndim)])\n",
    "    union = torch.count_nonzero(torch.logical_or(result, reference), dim=[i for i in range(1, result.ndim)])\n",
    "\n",
    "    iou = intersection.float() / union.float()\n",
    "\n",
    "    return iou.unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6a0cb3c68403dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, data_root, image_size=256, bbox_shift=5, data_aug=True):\n",
    "        self.data_root = data_root\n",
    "        self.gt_path = join(data_root, 'gts')\n",
    "        self.img_path = join(data_root, 'imgs')\n",
    "        self.gt_path_files = sorted(glob(join(self.gt_path, '*.npy'), recursive=True))\n",
    "        self.gt_path_files = [\n",
    "            file for file in self.gt_path_files\n",
    "            if isfile(join(self.img_path, basename(file)))\n",
    "        ]\n",
    "        self.image_size = image_size\n",
    "        self.target_length = image_size\n",
    "        self.bbox_shift = bbox_shift\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gt_path_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = basename(self.gt_path_files[index])\n",
    "        assert img_name == basename(self.gt_path_files[index]), 'img gt name error' + self.gt_path_files[index] + \\\n",
    "                                                                self.npy_files[index]\n",
    "        img_3c = np.load(join(self.img_path, img_name), 'r', allow_pickle=True)  # (H, W, 3)\n",
    "        img_resize = self.resize_longest_side(img_3c)\n",
    "        # Resizing\n",
    "        img_resize = (img_resize - img_resize.min()) / np.clip(img_resize.max() - img_resize.min(), a_min=1e-8,\n",
    "                                                               a_max=None)  # normalize to [0, 1], (H, W, 3\n",
    "        img_padded = self.pad_image(img_resize)  # (256, 256, 3)\n",
    "        # convert the shape to (3, H, W)\n",
    "        img_padded = np.transpose(img_padded, (2, 0, 1))  # (3, 256, 256)\n",
    "        assert np.max(img_padded) <= 1.0 and np.min(img_padded) >= 0.0, 'image should be normalized to [0, 1]'\n",
    "        gt = np.load(self.gt_path_files[index], 'r', allow_pickle=True)  # multiple labels [0, 1,4,5...], (256,256)\n",
    "        gt = cv2.resize(\n",
    "            gt,\n",
    "            (img_resize.shape[1], img_resize.shape[0]),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        ).astype(np.uint8)\n",
    "        gt = self.pad_image(gt)  # (256, 256)\n",
    "        label_ids = np.unique(gt)[1:]\n",
    "        try:\n",
    "            gt2D = np.uint8(gt == random.choice(label_ids.tolist()))  # only one label, (256, 256)\n",
    "        except:\n",
    "            print(img_name, 'label_ids.tolist()', label_ids.tolist())\n",
    "            gt2D = np.uint8(gt == np.max(gt))  # only one label, (256, 256)\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                img_padded = np.ascontiguousarray(np.flip(img_padded, axis=-1))\n",
    "                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-1))\n",
    "                # print('DA with flip left right')\n",
    "            if random.random() > 0.5:\n",
    "                img_padded = np.ascontiguousarray(np.flip(img_padded, axis=-2))\n",
    "                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-2))\n",
    "                # print('DA with flip upside down')\n",
    "        gt2D = np.uint8(gt2D > 0)\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = gt2D.shape\n",
    "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
    "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
    "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
    "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "        return {\n",
    "            \"image\": torch.tensor(img_padded).float(),\n",
    "            \"gt2D\": torch.tensor(gt2D[None, :, :]).long(),\n",
    "            \"bboxes\": torch.tensor(bboxes[None, None, ...]).float(),  # (B, 1, 4)\n",
    "            \"image_name\": img_name,\n",
    "            \"new_size\": torch.tensor(np.array([img_resize.shape[0], img_resize.shape[1]])).long(),\n",
    "            \"original_size\": torch.tensor(np.array([img_3c.shape[0], img_3c.shape[1]])).long()\n",
    "        }\n",
    "\n",
    "    def resize_longest_side(self, image):\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        long_side_length = self.target_length\n",
    "        oldh, oldw = image.shape[0], image.shape[1]\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww, newh = int(neww + 0.5), int(newh + 0.5)\n",
    "        target_size = (neww, newh)\n",
    "\n",
    "        return cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    def pad_image(self, image):\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        # Pad\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        padh = self.image_size - h\n",
    "        padw = self.image_size - w\n",
    "        if len(image.shape) == 3:  ## Pad image\n",
    "            image_padded = np.pad(image, ((0, padh), (0, padw), (0, 0)))\n",
    "        else:  ## Pad gt mask\n",
    "            image_padded = np.pad(image, ((0, padh), (0, padw)))\n",
    "\n",
    "        return image_padded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3a59db00c048058"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "class MedSAM_Lite(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_encoder,\n",
    "                 mask_decoder,\n",
    "                 prompt_encoder\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "    def forward(self, image, boxes):\n",
    "        image_embedding = self.image_encoder(image)  # (B, 256, 64, 64)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=boxes,\n",
    "            masks=None,\n",
    "        )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "        )  # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks, iou_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocess_masks(self, masks, new_size, original_size):\n",
    "        \"\"\"\n",
    "        Do cropping and resizing\n",
    "        \"\"\"\n",
    "        # Crop\n",
    "        masks = masks[:, :, :new_size[0], :new_size[1]]\n",
    "        # Resize\n",
    "        masks = F.interpolate(\n",
    "            masks,\n",
    "            size=(original_size[0], original_size[1]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb1bc625d0a6bc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "def main():\n",
    "    import wandb\n",
    "\n",
    "    # %%\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-task_name\", type=str, default=\"MedSAM-lite\")\n",
    "    parser.add_argument(\"-model_type\", type=str, default=\"vit_tiny\")\n",
    "    parser.add_argument(\n",
    "        \"-data_root\", type=str, default=\"./data/npy\",\n",
    "        help=\"Path to the npy data root.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-pretrained_checkpoint\", type=str, default=\"lite_medsam.pth\",\n",
    "        help=\"Path to the pretrained Lite-MedSAM checkpoint.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-resume\", type=str, default='workdir/medsam_lite_latest.pth',\n",
    "        help=\"Path to the checkpoint to continue training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        # \"-work_dir\", type=str, default=\"./workdir\",\n",
    "        \"-work_dir\", type=str, default=\"./work_dir\",\n",
    "        help=\"Path to the working directory where checkpoints and logs will be saved.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-num_epochs\", type=int, default=10,\n",
    "        help=\"Number of epochs to train.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-batch_size\", type=int, default=4,\n",
    "        help=\"Batch size.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-num_workers\", type=int, default=8,\n",
    "        help=\"Number of workers for dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-device\", type=str, default=\"mps\",\n",
    "        help=\"Device to train on.\"\n",
    "    )  # cuda:0\n",
    "    parser.add_argument(\n",
    "        \"-bbox_shift\", type=int, default=5,\n",
    "        help=\"Perturbation to bounding box coordinates during training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lr\", type=float, default=0.00005,\n",
    "        help=\"Learning rate.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-weight_decay\", type=float, default=0.01,\n",
    "        help=\"Weight decay.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-iou_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of IoU loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-seg_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of segmentation loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ce_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of cross entropy loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sanity_check\", action=\"store_true\", default=False,\n",
    "        help=\"Whether to do sanity check for dataloading.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-use_wandb\", type=bool, default=True,\n",
    "        help=\"use wandb to monitor training\"\n",
    "    )  # default=False\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=args.task_name,\n",
    "        name=\"run-6\",\n",
    "        config={\n",
    "            \"lr\": args.lr,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"data_path\": args.data_root,\n",
    "            \"epochs\": args.num_epochs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    model_save_path = join(args.work_dir, args.task_name + \"-\" + run_id)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    # %%\n",
    "    work_dir = args.work_dir\n",
    "    data_root = args.data_root\n",
    "    medsam_lite_checkpoint = args.pretrained_checkpoint\n",
    "    num_epochs = args.num_epochs\n",
    "    batch_size = args.batch_size\n",
    "    num_workers = args.num_workers\n",
    "    device = args.device\n",
    "    bbox_shift = args.bbox_shift\n",
    "    lr = args.lr\n",
    "    weight_decay = args.weight_decay\n",
    "    iou_loss_weight = args.iou_loss_weight\n",
    "    seg_loss_weight = args.seg_loss_weight\n",
    "    ce_loss_weight = args.ce_loss_weight\n",
    "    do_sancheck = args.sanity_check\n",
    "    checkpoint = args.resume\n",
    "\n",
    "    makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "    # %%\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # export OMP_NUM_THREADS=4\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  # export OPENBLAS_NUM_THREADS=4\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"6\"  # export MKL_NUM_THREADS=6\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  # export VECLIB_MAXIMUM_THREADS=4\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\"  # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "    # %% sanity test of dataset class\n",
    "    if do_sancheck:\n",
    "        tr_dataset = NpyDataset(data_root, data_aug=True)\n",
    "        tr_dataloader = DataLoader(tr_dataset, batch_size=8, shuffle=True)\n",
    "        for step, batch in enumerate(tr_dataloader):\n",
    "            # show the example\n",
    "            _, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "            idx = random.randint(0, 4)\n",
    "\n",
    "            image = batch[\"image\"]\n",
    "            gt = batch[\"gt2D\"]\n",
    "            bboxes = batch[\"bboxes\"]\n",
    "            names_temp = batch[\"image_name\"]\n",
    "\n",
    "            axs[0].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "            show_mask(gt[idx].cpu().squeeze().numpy(), axs[0])\n",
    "            show_box(bboxes[idx].numpy().squeeze(), axs[0])\n",
    "            axs[0].axis('off')\n",
    "            # set title\n",
    "            axs[0].set_title(names_temp[idx])\n",
    "            idx = random.randint(4, 7)\n",
    "            axs[1].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "            show_mask(gt[idx].cpu().squeeze().numpy(), axs[1])\n",
    "            show_box(bboxes[idx].numpy().squeeze(), axs[1])\n",
    "            axs[1].axis('off')\n",
    "            # set title\n",
    "            axs[1].set_title(names_temp[idx])\n",
    "            plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "            plt.savefig(\n",
    "                join(model_save_path, 'medsam_lite-train_bbox_prompt_sanitycheck_DA.png'),\n",
    "                bbox_inches='tight',\n",
    "                dpi=300\n",
    "            )\n",
    "            plt.close()\n",
    "            break\n",
    "\n",
    "    # %%\n",
    "    # medsam_lite_image_encoder = TinyViT(\n",
    "    medsam_lite_image_encoder = TinyViT(\n",
    "        img_size=256,\n",
    "        in_chans=3,\n",
    "        embed_dims=[\n",
    "            64,  ## (64, 256, 256)\n",
    "            128,  ## (128, 128, 128)\n",
    "            160,  ## (160, 64, 64)\n",
    "            320  ## (320, 64, 64)\n",
    "        ],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[2, 4, 5, 10],\n",
    "        window_sizes=[7, 7, 14, 7],\n",
    "        mlp_ratio=4.,\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.0,\n",
    "        use_checkpoint=False,\n",
    "        mbconv_expand_ratio=4.0,\n",
    "        local_conv_size=3,\n",
    "        layer_lr_decay=0.8\n",
    "    )\n",
    "\n",
    "    medsam_lite_prompt_encoder = PromptEncoder(\n",
    "        embed_dim=256,\n",
    "        image_embedding_size=(64, 64),\n",
    "        input_image_size=(256, 256),\n",
    "        mask_in_chans=16\n",
    "    )\n",
    "\n",
    "    medsam_lite_mask_decoder = MaskDecoder(\n",
    "        num_multimask_outputs=3,\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2,\n",
    "            embedding_dim=256,\n",
    "            mlp_dim=2048,\n",
    "            num_heads=8,\n",
    "        ),\n",
    "        transformer_dim=256,\n",
    "        iou_head_depth=3,\n",
    "        iou_head_hidden_dim=256,\n",
    "    )\n",
    "\n",
    "    medsam_lite_model = MedSAM_Lite(\n",
    "        image_encoder=medsam_lite_image_encoder,\n",
    "        mask_decoder=medsam_lite_mask_decoder,\n",
    "        prompt_encoder=medsam_lite_prompt_encoder\n",
    "    )\n",
    "\n",
    "    if medsam_lite_checkpoint is not None:\n",
    "        if isfile(medsam_lite_checkpoint):\n",
    "            print(f\"Finetuning with pretrained weights {medsam_lite_checkpoint}\")\n",
    "            medsam_lite_ckpt = torch.load(\n",
    "                medsam_lite_checkpoint,\n",
    "                map_location=\"cpu\"\n",
    "            )\n",
    "            medsam_lite_model.load_state_dict(medsam_lite_ckpt, strict=True)\n",
    "        else:\n",
    "            print(f\"Pretained weights {medsam_lite_checkpoint} not found, training from scratch\")\n",
    "\n",
    "    medsam_lite_model = medsam_lite_model.to(device)\n",
    "    medsam_lite_model.train()\n",
    "\n",
    "    # %%\n",
    "    print(f\"MedSAM Lite size: {sum(p.numel() for p in medsam_lite_model.parameters())}\")\n",
    "    # %%\n",
    "    optimizer = optim.AdamW(\n",
    "        medsam_lite_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.9,\n",
    "        patience=5,\n",
    "        cooldown=0\n",
    "    )\n",
    "    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "    ce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    iou_loss = nn.MSELoss(reduction='mean')\n",
    "    # %%\n",
    "    train_dataset = NpyDataset(data_root=data_root, data_aug=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              pin_memory=True)\n",
    "\n",
    "    if checkpoint and isfile(checkpoint):\n",
    "        print(f\"Resuming from checkpoint {checkpoint}\")\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        medsam_lite_model.load_state_dict(checkpoint[\"model\"], strict=True)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        best_loss = checkpoint[\"loss\"]\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(f\"if checkpoint else here\")\n",
    "        start_epoch = 0\n",
    "        best_loss = 1e10\n",
    "\n",
    "    train_losses = []\n",
    "    num_epochs = 10\n",
    "    for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "        print(f\"Starting epoch: {epoch}\")\n",
    "        print(f\"train_loader: {len(train_loader)}\")\n",
    "        epoch_loss = [1e10 for _ in range(len(train_loader))]\n",
    "        epoch_start_time = time()\n",
    "        pbar = tqdm(train_loader)\n",
    "        for step, batch in enumerate(pbar):\n",
    "            image = batch[\"image\"]\n",
    "            gt2D = batch[\"gt2D\"]\n",
    "            boxes = batch[\"bboxes\"]\n",
    "            optimizer.zero_grad()\n",
    "            image, gt2D, boxes = image.to(device), gt2D.to(device), boxes.to(device)\n",
    "            logits_pred, iou_pred = medsam_lite_model(image, boxes)\n",
    "            l_seg = seg_loss(logits_pred, gt2D)\n",
    "            l_ce = ce_loss(logits_pred, gt2D.float())\n",
    "            # mask_loss = l_seg + l_ce\n",
    "            mask_loss = seg_loss_weight * l_seg + ce_loss_weight * l_ce\n",
    "            iou_gt = cal_iou(torch.sigmoid(logits_pred) > 0.5, gt2D.bool())\n",
    "            l_iou = iou_loss(iou_pred, iou_gt)\n",
    "            # loss = mask_loss + l_iou\n",
    "            loss = mask_loss + iou_loss_weight * l_iou\n",
    "            epoch_loss[step] = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if args.use_wandb:\n",
    "            wandb.log({\"seg_loss_weight\": seg_loss_weight})\n",
    "            wandb.log({\"l_seg\": l_seg})\n",
    "            wandb.log({\"ce_loss_weight\": ce_loss_weight})\n",
    "            wandb.log({\"l_ce\": l_ce})\n",
    "            wandb.log({\"mask_loss\": mask_loss})\n",
    "\n",
    "            wandb.log({\"iou_gt\": iou_gt})\n",
    "\n",
    "            wandb.log({\"iou_loss_weight\": iou_loss_weight})\n",
    "            wandb.log({\"l_iou\": l_iou})\n",
    "            wandb.log({\"loss\": loss})\n",
    "\n",
    "            wandb.log({\"epoch_loss[step]\": epoch_loss[step]})\n",
    "\n",
    "            wandb.log({\"loss.item()\": loss.item()})\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_end_time = time()\n",
    "        epoch_loss_reduced = sum(epoch_loss) / len(epoch_loss)\n",
    "        wandb.log({\"epoch_loss\": epoch_loss_reduced})\n",
    "        train_losses.append(epoch_loss_reduced)\n",
    "        lr_scheduler.step(epoch_loss_reduced)\n",
    "        model_weights = medsam_lite_model.state_dict()\n",
    "        checkpoint = {\n",
    "            \"model\": model_weights,\n",
    "            \"epoch\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": epoch_loss_reduced,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, join(work_dir, \"medsam_lite_latest.pth\"))\n",
    "        if epoch_loss_reduced < best_loss:\n",
    "            print(f\"New best loss: {best_loss:.4f} -> {epoch_loss_reduced:.4f}\")\n",
    "            best_loss = epoch_loss_reduced\n",
    "            checkpoint[\"best_loss\"] = best_loss\n",
    "            torch.save(checkpoint, join(work_dir, \"medsam_lite_best.pth\"))\n",
    "        else:\n",
    "            print(f\"if epoch_loss_reduced < best_loss else\")\n",
    "            print(f\"New best loss: {best_loss:.4f}\")\n",
    "\n",
    "        epoch_loss_reduced = 1e10\n",
    "        # %% plot loss\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(\"Dice + Binary Cross Entropy + IoU Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.savefig(join(model_save_path, \"train_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Starting epoch: {epoch}\")\n",
    "\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7de1cfefc493fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7c02504d1b6e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
