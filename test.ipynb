{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from os import makedirs\n",
    "from os.path import join, isfile, basename\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n",
    "from tiny_vit_sam import TinyViT"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.45])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.45])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "\n",
    "def cal_iou(result, reference):\n",
    "    intersection = torch.count_nonzero(torch.logical_and(result, reference), dim=[i for i in range(1, result.ndim)])\n",
    "    union = torch.count_nonzero(torch.logical_or(result, reference), dim=[i for i in range(1, result.ndim)])\n",
    "\n",
    "    iou = intersection.float() / union.float()\n",
    "\n",
    "    return iou.unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6a0cb3c68403dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, data_root, image_size=256, bbox_shift=5, data_aug=True):\n",
    "        self.data_root = data_root\n",
    "        self.gt_path = join(data_root, 'gts')\n",
    "        self.img_path = join(data_root, 'imgs')\n",
    "        self.gt_path_files = sorted(glob(join(self.gt_path, '*.npy'), recursive=True))\n",
    "        self.gt_path_files = [\n",
    "            file for file in self.gt_path_files\n",
    "            if isfile(join(self.img_path, basename(file)))\n",
    "        ]\n",
    "        self.image_size = image_size\n",
    "        self.target_length = image_size\n",
    "        self.bbox_shift = bbox_shift\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gt_path_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = basename(self.gt_path_files[index])\n",
    "        assert img_name == basename(self.gt_path_files[index]), 'img gt name error' + self.gt_path_files[index] + \\\n",
    "                                                                self.npy_files[index]\n",
    "        img_3c = np.load(join(self.img_path, img_name), 'r', allow_pickle=True)  # (H, W, 3)\n",
    "        img_resize = self.resize_longest_side(img_3c)\n",
    "        # Resizing\n",
    "        img_resize = (img_resize - img_resize.min()) / np.clip(img_resize.max() - img_resize.min(), a_min=1e-8,\n",
    "                                                               a_max=None)  # normalize to [0, 1], (H, W, 3\n",
    "        img_padded = self.pad_image(img_resize)  # (256, 256, 3)\n",
    "        # convert the shape to (3, H, W)\n",
    "        img_padded = np.transpose(img_padded, (2, 0, 1))  # (3, 256, 256)\n",
    "        assert np.max(img_padded) <= 1.0 and np.min(img_padded) >= 0.0, 'image should be normalized to [0, 1]'\n",
    "        gt = np.load(self.gt_path_files[index], 'r', allow_pickle=True)  # multiple labels [0, 1,4,5...], (256,256)\n",
    "        gt = cv2.resize(\n",
    "            gt,\n",
    "            (img_resize.shape[1], img_resize.shape[0]),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        ).astype(np.uint8)\n",
    "        gt = self.pad_image(gt)  # (256, 256)\n",
    "        label_ids = np.unique(gt)[1:]\n",
    "        try:\n",
    "            gt2D = np.uint8(gt == random.choice(label_ids.tolist()))  # only one label, (256, 256)\n",
    "        except:\n",
    "            print(img_name, 'label_ids.tolist()', label_ids.tolist())\n",
    "            gt2D = np.uint8(gt == np.max(gt))  # only one label, (256, 256)\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                img_padded = np.ascontiguousarray(np.flip(img_padded, axis=-1))\n",
    "                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-1))\n",
    "                # print('DA with flip left right')\n",
    "            if random.random() > 0.5:\n",
    "                img_padded = np.ascontiguousarray(np.flip(img_padded, axis=-2))\n",
    "                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-2))\n",
    "                # print('DA with flip upside down')\n",
    "        gt2D = np.uint8(gt2D > 0)\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = gt2D.shape\n",
    "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
    "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
    "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
    "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "        return {\n",
    "            \"image\": torch.tensor(img_padded).float(),\n",
    "            \"gt2D\": torch.tensor(gt2D[None, :, :]).long(),\n",
    "            \"bboxes\": torch.tensor(bboxes[None, None, ...]).float(),  # (B, 1, 4)\n",
    "            \"image_name\": img_name,\n",
    "            \"new_size\": torch.tensor(np.array([img_resize.shape[0], img_resize.shape[1]])).long(),\n",
    "            \"original_size\": torch.tensor(np.array([img_3c.shape[0], img_3c.shape[1]])).long()\n",
    "        }\n",
    "\n",
    "    def resize_longest_side(self, image):\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        long_side_length = self.target_length\n",
    "        oldh, oldw = image.shape[0], image.shape[1]\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww, newh = int(neww + 0.5), int(newh + 0.5)\n",
    "        target_size = (neww, newh)\n",
    "\n",
    "        return cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    def pad_image(self, image):\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        # Pad\n",
    "        h, w = image.shape[0], image.shape[1]\n",
    "        padh = self.image_size - h\n",
    "        padw = self.image_size - w\n",
    "        if len(image.shape) == 3:  ## Pad image\n",
    "            image_padded = np.pad(image, ((0, padh), (0, padw), (0, 0)))\n",
    "        else:  ## Pad gt mask\n",
    "            image_padded = np.pad(image, ((0, padh), (0, padw)))\n",
    "\n",
    "        return image_padded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3a59db00c048058"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "class MedSAM_Lite(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_encoder,\n",
    "                 mask_decoder,\n",
    "                 prompt_encoder\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "    def forward(self, image, boxes):\n",
    "        image_embedding = self.image_encoder(image)  # (B, 256, 64, 64)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=boxes,\n",
    "            masks=None,\n",
    "        )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "        )  # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks, iou_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocess_masks(self, masks, new_size, original_size):\n",
    "        \"\"\"\n",
    "        Do cropping and resizing\n",
    "        \"\"\"\n",
    "        # Crop\n",
    "        masks = masks[:, :, :new_size[0], :new_size[1]]\n",
    "        # Resize\n",
    "        masks = F.interpolate(\n",
    "            masks,\n",
    "            size=(original_size[0], original_size[1]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb1bc625d0a6bc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%\n",
    "def main():\n",
    "    import wandb\n",
    "\n",
    "    # %%\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-task_name\", type=str, default=\"MedSAM-lite\")\n",
    "    parser.add_argument(\"-model_type\", type=str, default=\"vit_tiny\")\n",
    "    parser.add_argument(\n",
    "        \"-data_root\", type=str, default=\"./data/npy\",\n",
    "        help=\"Path to the npy data root.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-pretrained_checkpoint\", type=str, default=\"lite_medsam.pth\",\n",
    "        help=\"Path to the pretrained Lite-MedSAM checkpoint.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-resume\", type=str, default='workdir/medsam_lite_latest.pth',\n",
    "        help=\"Path to the checkpoint to continue training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        # \"-work_dir\", type=str, default=\"./workdir\",\n",
    "        \"-work_dir\", type=str, default=\"./work_dir\",\n",
    "        help=\"Path to the working directory where checkpoints and logs will be saved.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-num_epochs\", type=int, default=10,\n",
    "        help=\"Number of epochs to train.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-batch_size\", type=int, default=4,\n",
    "        help=\"Batch size.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-num_workers\", type=int, default=8,\n",
    "        help=\"Number of workers for dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-device\", type=str, default=\"mps\",\n",
    "        help=\"Device to train on.\"\n",
    "    )  # cuda:0\n",
    "    parser.add_argument(\n",
    "        \"-bbox_shift\", type=int, default=5,\n",
    "        help=\"Perturbation to bounding box coordinates during training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lr\", type=float, default=0.00005,\n",
    "        help=\"Learning rate.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-weight_decay\", type=float, default=0.01,\n",
    "        help=\"Weight decay.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-iou_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of IoU loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-seg_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of segmentation loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ce_loss_weight\", type=float, default=1.0,\n",
    "        help=\"Weight of cross entropy loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sanity_check\", action=\"store_true\", default=False,\n",
    "        help=\"Whether to do sanity check for dataloading.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-use_wandb\", type=bool, default=True,\n",
    "        help=\"use wandb to monitor training\"\n",
    "    )  # default=False\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=args.task_name,\n",
    "        name=\"run-6\",\n",
    "        config={\n",
    "            \"lr\": args.lr,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"data_path\": args.data_root,\n",
    "            \"epochs\": args.num_epochs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    model_save_path = join(args.work_dir, args.task_name + \"-\" + run_id)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    # %%\n",
    "    work_dir = args.work_dir\n",
    "    data_root = args.data_root\n",
    "    medsam_lite_checkpoint = args.pretrained_checkpoint\n",
    "    num_epochs = args.num_epochs\n",
    "    batch_size = args.batch_size\n",
    "    num_workers = args.num_workers\n",
    "    device = args.device\n",
    "    bbox_shift = args.bbox_shift\n",
    "    lr = args.lr\n",
    "    weight_decay = args.weight_decay\n",
    "    iou_loss_weight = args.iou_loss_weight\n",
    "    seg_loss_weight = args.seg_loss_weight\n",
    "    ce_loss_weight = args.ce_loss_weight\n",
    "    do_sancheck = args.sanity_check\n",
    "    checkpoint = args.resume\n",
    "\n",
    "    makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "    # %%\n",
    "    torch.cuda.empty_cache()\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # export OMP_NUM_THREADS=4\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"  # export OPENBLAS_NUM_THREADS=4\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"6\"  # export MKL_NUM_THREADS=6\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"  # export VECLIB_MAXIMUM_THREADS=4\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\"  # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "    # %% sanity test of dataset class\n",
    "    if do_sancheck:\n",
    "        tr_dataset = NpyDataset(data_root, data_aug=True)\n",
    "        tr_dataloader = DataLoader(tr_dataset, batch_size=8, shuffle=True)\n",
    "        for step, batch in enumerate(tr_dataloader):\n",
    "            # show the example\n",
    "            _, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "            idx = random.randint(0, 4)\n",
    "\n",
    "            image = batch[\"image\"]\n",
    "            gt = batch[\"gt2D\"]\n",
    "            bboxes = batch[\"bboxes\"]\n",
    "            names_temp = batch[\"image_name\"]\n",
    "\n",
    "            axs[0].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "            show_mask(gt[idx].cpu().squeeze().numpy(), axs[0])\n",
    "            show_box(bboxes[idx].numpy().squeeze(), axs[0])\n",
    "            axs[0].axis('off')\n",
    "            # set title\n",
    "            axs[0].set_title(names_temp[idx])\n",
    "            idx = random.randint(4, 7)\n",
    "            axs[1].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "            show_mask(gt[idx].cpu().squeeze().numpy(), axs[1])\n",
    "            show_box(bboxes[idx].numpy().squeeze(), axs[1])\n",
    "            axs[1].axis('off')\n",
    "            # set title\n",
    "            axs[1].set_title(names_temp[idx])\n",
    "            plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "            plt.savefig(\n",
    "                join(model_save_path, 'medsam_lite-train_bbox_prompt_sanitycheck_DA.png'),\n",
    "                bbox_inches='tight',\n",
    "                dpi=300\n",
    "            )\n",
    "            plt.close()\n",
    "            break\n",
    "\n",
    "    # %%\n",
    "    # medsam_lite_image_encoder = TinyViT(\n",
    "    medsam_lite_image_encoder = TinyViT(\n",
    "        img_size=256,\n",
    "        in_chans=3,\n",
    "        embed_dims=[\n",
    "            64,  ## (64, 256, 256)\n",
    "            128,  ## (128, 128, 128)\n",
    "            160,  ## (160, 64, 64)\n",
    "            320  ## (320, 64, 64)\n",
    "        ],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[2, 4, 5, 10],\n",
    "        window_sizes=[7, 7, 14, 7],\n",
    "        mlp_ratio=4.,\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.0,\n",
    "        use_checkpoint=False,\n",
    "        mbconv_expand_ratio=4.0,\n",
    "        local_conv_size=3,\n",
    "        layer_lr_decay=0.8\n",
    "    )\n",
    "\n",
    "    medsam_lite_prompt_encoder = PromptEncoder(\n",
    "        embed_dim=256,\n",
    "        image_embedding_size=(64, 64),\n",
    "        input_image_size=(256, 256),\n",
    "        mask_in_chans=16\n",
    "    )\n",
    "\n",
    "    medsam_lite_mask_decoder = MaskDecoder(\n",
    "        num_multimask_outputs=3,\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2,\n",
    "            embedding_dim=256,\n",
    "            mlp_dim=2048,\n",
    "            num_heads=8,\n",
    "        ),\n",
    "        transformer_dim=256,\n",
    "        iou_head_depth=3,\n",
    "        iou_head_hidden_dim=256,\n",
    "    )\n",
    "\n",
    "    medsam_lite_model = MedSAM_Lite(\n",
    "        image_encoder=medsam_lite_image_encoder,\n",
    "        mask_decoder=medsam_lite_mask_decoder,\n",
    "        prompt_encoder=medsam_lite_prompt_encoder\n",
    "    )\n",
    "\n",
    "    if medsam_lite_checkpoint is not None:\n",
    "        if isfile(medsam_lite_checkpoint):\n",
    "            print(f\"Finetuning with pretrained weights {medsam_lite_checkpoint}\")\n",
    "            medsam_lite_ckpt = torch.load(\n",
    "                medsam_lite_checkpoint,\n",
    "                map_location=\"cpu\"\n",
    "            )\n",
    "            medsam_lite_model.load_state_dict(medsam_lite_ckpt, strict=True)\n",
    "        else:\n",
    "            print(f\"Pretained weights {medsam_lite_checkpoint} not found, training from scratch\")\n",
    "\n",
    "    medsam_lite_model = medsam_lite_model.to(device)\n",
    "    medsam_lite_model.train()\n",
    "\n",
    "    # %%\n",
    "    print(f\"MedSAM Lite size: {sum(p.numel() for p in medsam_lite_model.parameters())}\")\n",
    "    # %%\n",
    "    optimizer = optim.AdamW(\n",
    "        medsam_lite_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.9,\n",
    "        patience=5,\n",
    "        cooldown=0\n",
    "    )\n",
    "    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "    ce_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    iou_loss = nn.MSELoss(reduction='mean')\n",
    "    # %%\n",
    "    train_dataset = NpyDataset(data_root=data_root, data_aug=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              pin_memory=True)\n",
    "\n",
    "    if checkpoint and isfile(checkpoint):\n",
    "        print(f\"Resuming from checkpoint {checkpoint}\")\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        medsam_lite_model.load_state_dict(checkpoint[\"model\"], strict=True)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        best_loss = checkpoint[\"loss\"]\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(f\"if checkpoint else here\")\n",
    "        start_epoch = 0\n",
    "        best_loss = 1e10\n",
    "\n",
    "    train_losses = []\n",
    "    num_epochs = 10\n",
    "    for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "        print(f\"Starting epoch: {epoch}\")\n",
    "        print(f\"train_loader: {len(train_loader)}\")\n",
    "        epoch_loss = [1e10 for _ in range(len(train_loader))]\n",
    "        epoch_start_time = time()\n",
    "        pbar = tqdm(train_loader)\n",
    "        for step, batch in enumerate(pbar):\n",
    "            image = batch[\"image\"]\n",
    "            gt2D = batch[\"gt2D\"]\n",
    "            boxes = batch[\"bboxes\"]\n",
    "            optimizer.zero_grad()\n",
    "            image, gt2D, boxes = image.to(device), gt2D.to(device), boxes.to(device)\n",
    "            logits_pred, iou_pred = medsam_lite_model(image, boxes)\n",
    "            l_seg = seg_loss(logits_pred, gt2D)\n",
    "            l_ce = ce_loss(logits_pred, gt2D.float())\n",
    "            # mask_loss = l_seg + l_ce\n",
    "            mask_loss = seg_loss_weight * l_seg + ce_loss_weight * l_ce\n",
    "            iou_gt = cal_iou(torch.sigmoid(logits_pred) > 0.5, gt2D.bool())\n",
    "            l_iou = iou_loss(iou_pred, iou_gt)\n",
    "            # loss = mask_loss + l_iou\n",
    "            loss = mask_loss + iou_loss_weight * l_iou\n",
    "            epoch_loss[step] = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # if args.use_wandb:\n",
    "            wandb.log({\"seg_loss_weight\": seg_loss_weight})\n",
    "            wandb.log({\"l_seg\": l_seg})\n",
    "            wandb.log({\"ce_loss_weight\": ce_loss_weight})\n",
    "            wandb.log({\"l_ce\": l_ce})\n",
    "            wandb.log({\"mask_loss\": mask_loss})\n",
    "\n",
    "            wandb.log({\"iou_gt\": iou_gt})\n",
    "\n",
    "            wandb.log({\"iou_loss_weight\": iou_loss_weight})\n",
    "            wandb.log({\"l_iou\": l_iou})\n",
    "            wandb.log({\"loss\": loss})\n",
    "\n",
    "            wandb.log({\"epoch_loss[step]\": epoch_loss[step]})\n",
    "\n",
    "            wandb.log({\"loss.item()\": loss.item()})\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_end_time = time()\n",
    "        epoch_loss_reduced = sum(epoch_loss) / len(epoch_loss)\n",
    "        wandb.log({\"epoch_loss\": epoch_loss_reduced})\n",
    "        train_losses.append(epoch_loss_reduced)\n",
    "        lr_scheduler.step(epoch_loss_reduced)\n",
    "        model_weights = medsam_lite_model.state_dict()\n",
    "        checkpoint = {\n",
    "            \"model\": model_weights,\n",
    "            \"epoch\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": epoch_loss_reduced,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, join(work_dir, \"medsam_lite_latest.pth\"))\n",
    "        if epoch_loss_reduced < best_loss:\n",
    "            print(f\"New best loss: {best_loss:.4f} -> {epoch_loss_reduced:.4f}\")\n",
    "            best_loss = epoch_loss_reduced\n",
    "            checkpoint[\"best_loss\"] = best_loss\n",
    "            torch.save(checkpoint, join(work_dir, \"medsam_lite_best.pth\"))\n",
    "        else:\n",
    "            print(f\"if epoch_loss_reduced < best_loss else\")\n",
    "            print(f\"New best loss: {best_loss:.4f}\")\n",
    "\n",
    "        epoch_loss_reduced = 1e10\n",
    "        # %% plot loss\n",
    "        plt.plot(train_losses)\n",
    "        plt.title(\"Dice + Binary Cross Entropy + IoU Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.savefig(join(model_save_path, \"train_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Starting epoch: {epoch}\")\n",
    "\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7de1cfefc493fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7c02504d1b6e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
